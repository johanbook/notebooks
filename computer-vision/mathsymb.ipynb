{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a notebook for training a MNIST frequentist model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from torch import save as save_state_dict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loaders\n",
    "There are several datasets available for digit (and letter) classification. Standard is MNIST. There is also EMNIST, also has letters. There is QMNIST which is basically MNIST but larger.\n",
    "\n",
    "We will also here use data augmentation (transforms.RandomAffine) in order for the model to generalize better. Note that we apply the data augmentation to training aswell as validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loaders(split='digits'):\n",
    "    transform = transforms.Compose([\n",
    "        #transforms.RandomAffine(degrees=30, translate=(0.2, 0.2), scale=(0.7, 1.2)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    trainset = torchvision.datasets.EMNIST(\n",
    "        root='/storage',\n",
    "        split=split,\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transform,\n",
    "    )\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        trainset,\n",
    "        batch_size=100,\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    testset = torchvision.datasets.EMNIST(\n",
    "        root='/storage',\n",
    "        split=split,\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transform,\n",
    "    )\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset,\n",
    "        batch_size=100,\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make som plots to see how data augmentation works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAANy0lEQVR4nO3dX4hd5bnH8d/PqBhjNWqGMGo8U6sgEtTWMQQq1UO1am5Mb0JzURWE9MJIC72o9FzUSyk2peChkB7FePAohfonF+GcmlCUQgwZQ4yJQU0k0oTEGYl/UpIYJz7nYlZkGme/a9xr/4vP9wPD3ns9e2U/bvPL2rPe/a7XESEA33xn9bsBAL1B2IEkCDuQBGEHkiDsQBJn9/LFFixYECMjI718SSCVffv26cMPP/RMtUZht32XpD9ImiPpvyLi0dLzR0ZGNDY21uQlARSMjo62rLX9Md72HEn/KeluSddJWmn7unb/PADd1eR39iWS9kTEexFxQtJzku7pTFsAOq1J2C+X9I9pj/dX2/6F7VW2x2yPTUxMNHg5AE10/Wx8RKyNiNGIGB0aGur2ywFooUnYD0haNO3xFdU2AAOoSdi3SrrG9rdtnyvpJ5LWd6YtAJ3W9tBbREzaXi3p/zQ19PZkROzqWGcAOqrROHtEbJC0oUO9AOgivi4LJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSKLRks2290k6IumkpMmIGO1EUwA6r1HYK/8eER924M8B0EV8jAeSaBr2kPRX26/bXjXTE2yvsj1me2xiYqLhywFoV9Ow3xIR35N0t6QHbf/g9CdExNqIGI2I0aGhoYYvB6BdjcIeEQeq23FJL0ha0ommAHRe22G3Pc/2t07dl/QjSTs71RiAzmpyNn6hpBdsn/pz/ici/rcjXQHouLbDHhHvSbqhg70A6CKG3oAkCDuQBGEHkiDsQBKEHUiiExNhgIEUES1rX3zxRXHfuvrx48eL9blz5xbrc+bMaVmrhrM7jiM7kARhB5Ig7EAShB1IgrADSRB2IAnCDiTBODvOWJOTk8X6iRMnWtaOHj1a3PfYsWPF+q5du4r1pUuXFusXXHBBy9rZZ3cnlhzZgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJxtnRVaU55aWaVD/WvWXLlmJ9w4YNLWsbN25s9Noff/xxsf74448X63fccUfL2vz584v7tosjO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kwTg7GmkyVv7ZZ58V93377beL9WeeeaZY37x5c8va3r17i/vWXTd+3rx5jfbvh9oju+0nbY/b3jlt2yW2X7b9bnV7cXfbBNDUbD7GPyXprtO2PSxpU0RcI2lT9RjAAKsNe0S8KunwaZvvkbSuur9O0vIO9wWgw9o9QbcwIg5W9w9JWtjqibZX2R6zPTYxMdHmywFoqvHZ+Jg6Q9PyLE1ErI2I0YgYHRoaavpyANrUbtg/sD0sSdXteOdaAtAN7YZ9vaT7qvv3SXqpM+0A6JbacXbbz0q6TdIC2/sl/UbSo5L+bPsBSe9LWtHNJtE9dePBJ0+eLNbrzsOsWbOmZW3btm3FfXfv3t3otUvfATjvvPOK+9bNKV+5cmWxfueddxbrpevGd0tt2COi1X/VDzvcC4Au4uuyQBKEHUiCsANJEHYgCcIOJMEU12+4zz//vFh/4403ivWtW7cW608//XTbf37dFNe66bN100wXLVrUsrZ69erivjfddFOxvnjx4mL9/PPPL9ZtF+vdwJEdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5JgnH0A1I0n101DLdXrpoE+99xzxfprr71WrO/YsaNYP378eLFeMnfu3GL9hhtuKNaXLl3asrZ8efmyiRdddFGxXtdbP8bR63BkB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkGGfvgMnJyWK9bt72zp07i/W6Sy5/+umnLWsvvvhicd+6+ex1vdd9B6A0r/vKK68s7vvQQw8V603Gygdxvnm3cWQHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQYZ6/UzSkv1Y8cOVLcd+/evcX6U089Vaxv3ry5WD927FjL2v79+4v7NplvLknnnHNOsT4yMtKydvvttxf3rRtHHxoaKtbnzJnTsvZNHEevU3tkt/2k7XHbO6dte8T2Advbq59l3W0TQFOz+Rj/lKS7Ztj++4i4sfrZ0Nm2AHRabdgj4lVJh3vQC4AuanKCbrXtHdXH/ItbPcn2KttjtsfqrocGoHvaDfsfJX1H0o2SDkr6XasnRsTaiBiNiNG6EyoAuqetsEfEBxFxMiK+kPQnSUs62xaATmsr7LaHpz38saTyHE0AfVc7zm77WUm3SVpge7+k30i6zfaNkkLSPkk/62KPs1I3Tn706NFivW5O+Z49e1rWNm7cWNx3y5Ytxfo777xTrNfNGW+iyRrnknT//fcX66Wx9Kuuuqq4b9212zOOlTdRG/aIWDnD5ie60AuALuLrskAShB1IgrADSRB2IAnCDiRxRk1xLV3W+NChQ8V9H3vssWL9+eefL9Y/+eSTlrW6aaJ1Q2dnnVX+N3fhwoXF+oUXXtiydtlllxX3XbFiRbF+8803F+vXX399sV6aAsvQWW9xZAeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJM6ocfbSePXhw+XL5L3yyivF+vj4eLF+8uTJYr2k6Tj6smXli/deffXVLWt1yyLfeuutxXrdNNO6S0kzlj44OLIDSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBJn1Dh7acx27ty5xX2vuOKKTrfzpUsvvbRYv/baa4v1ussxL168uFg/99xzW9ZKyxZL9d8BwDcH/6eBJAg7kARhB5Ig7EAShB1IgrADSRB2IIkzapy9NHd6eHi4ZU2S7r333mL9o48+aqsnSZo/f36xXrc0cd04et13CBgrx2zU/i2xvcj232y/ZXuX7Z9X2y+x/bLtd6vbi7vfLoB2zeaQMCnplxFxnaSlkh60fZ2khyVtiohrJG2qHgMYULVhj4iDEbGtun9E0m5Jl0u6R9K66mnrJC3vVpMAmvtav+zZHpH0XUlbJC2MiINV6ZCkGS+kZnuV7THbYxMTEw1aBdDErMNu+wJJf5H0i4j4dHotIkJSzLRfRKyNiNGIGB0aGmrULID2zSrsts/RVNCfiYhTy51+YHu4qg9LKl+eFUBf1Q69eWpe6ROSdkfEmmml9ZLuk/RodftSVzqcpjRds+6Sx3VLE3dT3eWUudwyemE24+zfl/RTSW/a3l5t+7WmQv5n2w9Iel9S/9IEoFZt2CPi75JaHXp+2Nl2AHQLX70CkiDsQBKEHUiCsANJEHYgiTNqimsTTANFdiQASIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSqA277UW2/2b7Ldu7bP+82v6I7QO2t1c/y7rfLoB2zWaRiElJv4yIbba/Jel12y9Xtd9HxGPdaw9Ap8xmffaDkg5W94/Y3i3p8m43BqCzvtbv7LZHJH1X0pZq02rbO2w/afviFvussj1me2xiYqJRswDaN+uw275A0l8k/SIiPpX0R0nfkXSjpo78v5tpv4hYGxGjETE6NDTUgZYBtGNWYbd9jqaC/kxEPC9JEfFBRJyMiC8k/UnSku61CaCp2ZyNt6QnJO2OiDXTtg9Pe9qPJe3sfHsAOmU2Z+O/L+mnkt60vb3a9mtJK23fKCkk7ZP0s650CKAjZnM2/u+SPENpQ+fbAdAtfIMOSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQhCOidy9mT0h6f9qmBZI+7FkDX8+g9jaofUn01q5O9vZvETHj9d96GvavvLg9FhGjfWugYFB7G9S+JHprV69642M8kARhB5Lod9jX9vn1Swa1t0HtS6K3dvWkt77+zg6gd/p9ZAfQI4QdSKIvYbd9l+23be+x/XA/emjF9j7bb1bLUI/1uZcnbY/b3jlt2yW2X7b9bnU74xp7feptIJbxLiwz3tf3rt/Ln/f8d3bbcyS9I+kOSfslbZW0MiLe6mkjLdjeJ2k0Ivr+BQzbP5D0T0lPR8TiattvJR2OiEerfygvjohfDUhvj0j6Z7+X8a5WKxqevsy4pOWS7lcf37tCXyvUg/etH0f2JZL2RMR7EXFC0nOS7ulDHwMvIl6VdPi0zfdIWlfdX6epvyw916K3gRARByNiW3X/iKRTy4z39b0r9NUT/Qj75ZL+Me3xfg3Weu8h6a+2X7e9qt/NzGBhRBys7h+StLCfzcygdhnvXjptmfGBee/aWf68KU7QfdUtEfE9SXdLerD6uDqQYup3sEEaO53VMt69MsMy41/q53vX7vLnTfUj7AckLZr2+Ipq20CIiAPV7bikFzR4S1F/cGoF3ep2vM/9fGmQlvGeaZlxDcB718/lz/sR9q2SrrH9bdvnSvqJpPV96OMrbM+rTpzI9jxJP9LgLUW9XtJ91f37JL3Ux17+xaAs491qmXH1+b3r+/LnEdHzH0nLNHVGfq+k/+hHDy36ukrSG9XPrn73JulZTX2s+1xT5zYekHSppE2S3pW0UdIlA9Tbf0t6U9IOTQVruE+93aKpj+g7JG2vfpb1+70r9NWT942vywJJcIIOSIKwA0kQdiAJwg4kQdiBJAg7kARhB5L4f8HRVicu2y96AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_data_augmentation():\n",
    "    train_loader, _ = get_data_loaders('letters')\n",
    "    for data, labels in train_loader:\n",
    "        sample = data[0, :, :, :].view(28, 28).numpy()\n",
    "        plt.imshow(sample, cmap='Greys')\n",
    "        plt.show()\n",
    "        break\n",
    "plot_data_augmentation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "Define some metrics so we have an easy way to measure accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader_accuracy(loader, model):\n",
    "    with torch.no_grad():\n",
    "        cum_acc = 0\n",
    "        for x, y in loader:\n",
    "            cum_acc += accuracy(x.to(DEVICE), y.to(DEVICE), model)\n",
    "        return cum_acc / len(loader)\n",
    "\n",
    "\n",
    "def accuracy(x, y, model):\n",
    "    with torch.no_grad():\n",
    "        pred = model(x).argmax(dim=1, keepdim=True)\n",
    "        corrects = pred.eq(y.view_as(pred)).sum().item()\n",
    "        acc = corrects * 100 / len(x)\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "Here is our model. It is quite complex, but since we usse quite aggressive data augmentation, we run little risk of overtrainng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size()[0], -1)\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.arch = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=5, stride=1),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            # 4x4 image\n",
    "\n",
    "            Flatten(),\n",
    "            nn.Linear(64*4*4, 512),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.Linear(256, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.arch(x), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our main training loop. It also produces some nice plots when it is done :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(epochs=20, optim=torch.optim.Adam, lr=1e-3):\n",
    "    model = CNN()\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    losses = []\n",
    "    val_accs = []\n",
    "    train_accs = []\n",
    "    \n",
    "    train_loader, test_loader = get_data_loaders()\n",
    "    optim = optim(model.parameters(), lr=lr)\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for data, labels in train_loader:\n",
    "            preds = model(data.to(DEVICE))\n",
    "            loss = F.cross_entropy(preds, labels)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "            total_loss += loss.item() / len(labels)\n",
    "            \n",
    "        # Multiply loss by 100 to make it more relatable. NB: This is not a best practice\n",
    "        total_loss *= 100/len(train_loader)\n",
    "        losses.append(total_loss)\n",
    "        val_acc = loader_accuracy(test_loader, model)\n",
    "        val_accs.append(val_acc)\n",
    "        train_acc = loader_accuracy(train_loader, model)\n",
    "        train_accs.append(train_acc)\n",
    "        \n",
    "        print(f'EPOCH {epoch+1}, validation accuracy {val_acc:.2f}% loss {total_loss:.2f}')\n",
    "    \n",
    "    plt.figure(figsize=(15, 5), dpi= 80, facecolor='w', edgecolor='k')\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_accs, label='Training')\n",
    "    plt.plot(val_accs, label='Validation')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Validation')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Training loss')\n",
    "    plt.show()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now actually create and train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save it\n",
    "Save model statedict so we can use it elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(model, path='mnist.pt'):\n",
    "    save_state_dict(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Torch)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
